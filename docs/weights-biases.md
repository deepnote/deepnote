---
title: Weights & Biases
noIndex: false
noContent: false
---

`#` Weights & Biases

In Deepnote, **Weights & Biases** (W&B) plays a key role in helping developers track, visualize, and improve machine learning models. W&B is seamlessly integrated into the workflow, allowing users to follow the results of experiments, track hyperparameters, and visualize training metrics in real time. Its easy integration and comprehensive toolset make W&B a key component in any machine learning project.

Weights & Biases covers a wide range of resources in its [Tutorials section](https://docs.wandb.ai/tutorials) for those interested in learning more about improving their machine learning workflows. For those working with PyTorch, there are dedicated [PyTorch tutorials](https://docs.wandb.ai/tutorials/pytorch) to help them get started.

##Key capabilities of Weights & Biases

Weights & Biases is a machine learning platform designed to accelerate the development and deployment of better models. The platform offers a lightweight, interactive toolkit that allows developers to:

- **Track experiments:** W&B captures and records data from experiments in real time, helping users track model performance over multiple iterations.

- **Version and iterate datasets:** Users can easily manage and version their datasets to ensure consistency across experiments.

- **Evaluate model performance:** W&B provides detailed visualizations of model metrics, allowing users to monitor accuracy, loss, and other performance indicators.

- **Reproduce models:** It ensures reproducibility by documenting configurations, environments, and code versions.

- **Visualization of results:** An intuitive interface allows rich visualizations to better understand model behavior.

- **Spot Regressions:** W&B helps identify underperforming areas of models, allowing developers to solve problems more efficiently

- **Share findings:** Users can easily share results and insights with colleagues or the broader community.

##Why choose Weights and Biases?

Compared to other experiment tracking platforms like [Comet.ml](https://deepnote.com/docs/cometml), Weights & Biases stands out with its easy integration, powerful visualization tools, and comprehensive teaching support. W&B is lightweight and doesn't add overhead to existing workflows. Its robust feature set for version control, duplication, and sharing makes it a comprehensive tool for both individual developers and larger teams.

For more details on leveraging Weights & Biases for machine learning projects, [the documentation](https://docs.wandb.ai/tutorials) provides comprehensive guides and tutorials.

##Using Weights & Biases in Deepnote

Wandb can be easily integrated into Deepnote by following these steps:

```
!pip install --upgradeÂ wandb
!wandb login <your_wandb_token>
```

The W&B token can be generated by visiting the [Weights & Biases login page](https://wandb.ai/login). Once logged i, the token can be copied from your W&B account settings and used in Deepnote's terminal. The same steps can be completed directly in Deepnote's terminal, making set up easy.

For illustration, here's an example of using W&B in Deepnote: [2024 Olympic predictor: powered by PyTorch and W&B ](https://deepnote.com/app/deepnote/olympic-medal-predictions-50996ef9-840d-41df-9529-21024823f254?input_1=Azerbaijan&Team=Great+Britain), where an ML model was trained using PyTorch.

###A few examples of how W&B can be used:

- **Training with W&B:** This code snippet runs a training loop, where the model predicts, computes loss, updates parameters, and adjusts the learning rate each epoch. Weights and Biases tracks the experiment configuration.

```
#Training loop with learning rate scheduling
num_epochs = wandb.config.epochs
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    scheduler.step()
```

- **Logging and monitoring loss:** This code logs training loss to W&B for performance tracking and prints the loss every 10 epochs to monitor progress during training.

```
# Log training loss to W&B
wandb.log({"loss": loss.item()})

if (epoch + 1) % 10 == 0:
    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')
```

- **Logging test accuracy:** In this code snippet the W&B logs the test accuracy after each epoch, enabling performance tracking on the test set.

```
# Log test accuracy to W&B
wandb.log({"test_accuracy": test_accuracy})
```

In the log of a specific notebook or in a personal workspace on the W&B website, various characteristics of concrete project can be seen. Below are two figures showing an example of the progress of 10 runs related to loss and accuracy.

![W&B Chart 12. 9. 2024 11_53_19.png](https://media.graphassets.com/eTSQgjnfSnGxFNq8hUgi)

![W&B Chart 12. 9. 2024 11_52_48.png](https://media.graphassets.com/kxCCl6nQYW7CuZ0jxVds)
##Conclusion

Weights & Biases is a powerful machine learning platform that simplifies experiment tracking, model evaluation and collaboration. Seamless integration with Deepnote provides a comprehensive solution for developers looking to build and refine models. Whether it's a through an simple setup process or powerful visualization tools, W&B helps users build better models faster.
